{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tqdm\n",
    "import pdb\n",
    "from scipy.sparse import csr_matrix, linalg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users shape:  (68092, 3)\n",
      "books shape:  (149570, 10)\n",
      "train_ratings shape:  (306795, 3)\n"
     ]
    }
   ],
   "source": [
    "path= '/opt/ml/level1_bookratingprediction_recsys-level1-recsys-06/data/'\n",
    "\n",
    "users = pd.read_csv(path+'users.csv')\n",
    "books = pd.read_csv(path+'books.csv')\n",
    "train_ratings = pd.read_csv(path+'train_ratings.csv')\n",
    "test_ratings = pd.read_csv(path+'test_ratings.csv')\n",
    "submission = pd.read_csv(path + 'sample_submission.csv')\n",
    "\n",
    "print('users shape: ', users.shape)\n",
    "print('books shape: ', books.shape)\n",
    "print('train_ratings shape: ', train_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(real: list, predict: list) -> float:\n",
    "    pred = np.array(predict)\n",
    "    return np.sqrt(np.mean((real-pred) ** 2))\n",
    "\n",
    "SEED = 42\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = train['user_id'].value_counts()\n",
    "tem = list(tem[tem >= 10].index)\n",
    "train['id'] = train['user_id'].copy()\n",
    "train['id'][~train['id'].isin(tem)] = -1\n",
    "test['id'] = test['user_id'].copy()\n",
    "test['id'][~test['id'].isin(tem)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.concat([train_ratings, test_ratings]).reset_index(drop=True)\n",
    "context_df = ratings.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "train_df = train_ratings.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "test_df = test_ratings.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_city2idx = {v:k for k,v in enumerate(context_df['location_city'].unique())}\n",
    "loc_state2idx = {v:k for k,v in enumerate(context_df['location_state'].unique())}\n",
    "loc_country2idx = {v:k for k,v in enumerate(context_df['location_country'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['location_city'] = train_df['location_city'].map(loc_city2idx)\n",
    "train_df['location_state'] = train_df['location_state'].map(loc_state2idx)\n",
    "train_df['location_country'] = train_df['location_country'].map(loc_country2idx)\n",
    "\n",
    "test_df['location_city'] = test_df['location_city'].map(loc_city2idx)\n",
    "test_df['location_state'] = test_df['location_state'].map(loc_state2idx)\n",
    "test_df['location_country'] = test_df['location_country'].map(loc_country2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_map(x: int) -> int:\n",
    "    x = int(x)\n",
    "    if x < 20:\n",
    "        return 1\n",
    "    elif x >= 20 and x < 30:\n",
    "        return 2\n",
    "    elif x >= 30 and x < 40:\n",
    "        return 3\n",
    "    elif x >= 40 and x < 50:\n",
    "        return 4\n",
    "    elif x >= 50 and x < 60:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['age'] = train_df['age'].fillna(int(train_df['age'].mean()))\n",
    "train_df['age'] = train_df['age'].apply(age_map)\n",
    "test_df['age'] = test_df['age'].fillna(int(test_df['age'].mean()))\n",
    "test_df['age'] = test_df['age'].apply(age_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "category2idx = {v:k for k,v in enumerate(context_df['category'].unique())}\n",
    "publisher2idx = {v:k for k,v in enumerate(context_df['publisher'].unique())}\n",
    "language2idx = {v:k for k,v in enumerate(context_df['language'].unique())}\n",
    "author2idx = {v:k for k,v in enumerate(context_df['book_author'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['category'] = train_df['category'].map(category2idx)\n",
    "train_df['publisher'] = train_df['publisher'].map(publisher2idx)\n",
    "train_df['language'] = train_df['language'].map(language2idx)\n",
    "train_df['book_author'] = train_df['book_author'].map(author2idx)\n",
    "\n",
    "test_df['category'] = test_df['category'].map(category2idx)\n",
    "test_df['publisher'] = test_df['publisher'].map(publisher2idx)\n",
    "test_df['language'] = test_df['language'].map(language2idx)\n",
    "test_df['book_author'] = test_df['book_author'].map(author2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['isbn'] = train_df['isbn'].astype('category')\n",
    "test_df['isbn'] = test_df['isbn'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 306795 entries, 0 to 306794\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count   Dtype   \n",
      "---  ------            --------------   -----   \n",
      " 0   user_id           306795 non-null  int64   \n",
      " 1   isbn              306795 non-null  category\n",
      " 2   rating            306795 non-null  int64   \n",
      " 3   age               306795 non-null  int64   \n",
      " 4   location_city     306795 non-null  int64   \n",
      " 5   location_state    306795 non-null  int64   \n",
      " 6   location_country  306795 non-null  int64   \n",
      " 7   category          306795 non-null  int64   \n",
      " 8   publisher         306795 non-null  int64   \n",
      " 9   language          306795 non-null  int64   \n",
      " 10  book_author       306795 non-null  int64   \n",
      "dtypes: category(1), int64(10)\n",
      "memory usage: 31.9 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_df.drop(['rating'], axis = 1), train_df['rating'], test_size=0.2, random_state=42)\n",
    "train_ds = lgb.Dataset(train_x, label = train_y) \n",
    "val_ds = lgb.Dataset(val_x, label = val_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.01, \n",
    "          'max_depth': 16, \n",
    "          'boosting': 'gbdt', \n",
    "          'objective': 'regression', \n",
    "          'metric': 'rmse', \n",
    "          'is_training_metric': True, \n",
    "          'num_leaves': 144, \n",
    "          'feature_fraction': 0.9, \n",
    "          'bagging_fraction': 0.7, \n",
    "          'bagging_freq': 5, \n",
    "          'seed':42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 15800\n",
      "[LightGBM] [Info] Number of data points in the train set: 245436, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 7.068747\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's rmse: 2.35352\n",
      "[200]\tvalid_0's rmse: 2.3377\n",
      "[300]\tvalid_0's rmse: 2.32986\n",
      "[400]\tvalid_0's rmse: 2.32636\n",
      "[500]\tvalid_0's rmse: 2.3213\n",
      "[600]\tvalid_0's rmse: 2.3167\n",
      "[700]\tvalid_0's rmse: 2.31337\n",
      "[800]\tvalid_0's rmse: 2.31196\n",
      "[900]\tvalid_0's rmse: 2.30973\n",
      "[1000]\tvalid_0's rmse: 2.30709\n",
      "[1100]\tvalid_0's rmse: 2.30593\n",
      "[1200]\tvalid_0's rmse: 2.30512\n",
      "[1300]\tvalid_0's rmse: 2.30456\n",
      "[1400]\tvalid_0's rmse: 2.30326\n",
      "[1500]\tvalid_0's rmse: 2.30175\n",
      "[1600]\tvalid_0's rmse: 2.30064\n",
      "[1700]\tvalid_0's rmse: 2.29969\n",
      "[1800]\tvalid_0's rmse: 2.29941\n",
      "[1900]\tvalid_0's rmse: 2.2985\n",
      "[2000]\tvalid_0's rmse: 2.2978\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1986]\tvalid_0's rmse: 2.2977\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(params, train_ds, 2000, val_ds, verbose_eval=100, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
